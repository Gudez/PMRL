import torch
import torch.nn.functional as F
import torch.nn as nn
from torchsummary import summary
from Guderzo_encoding import convert_to_hot
import random

train_matrix, char_to_int, int_to_char = convert_to_hot("C:/Users/ricca/Desktop/RGud/TAMPERE_UNIVERSITY/2_SEMESTER/PATTERN RECOGNITION AND MACHINE LEARNING/EXERCISE5/abcde.txt")


def train_test_data(original_matrix=torch.rand(1, 7), index_from=0,
                    index_to=1):
    """
    Given a dataset, split it into X and Y based on the request.
    :param original_matrix:
    :param index_from:
    :param index_to:
    :return: X, Y training data
    """
    X = original_matrix[index_from:index_to]
    Y = original_matrix[index_from + 1:index_to + 1]

    return X, Y

def evaluation(X):
    """
    Given a matrix of data, randomly choose a char and print the next 50 chars
    generated by the NN
    :param X:
    :return:
    """
    # print(X[0:20])
    rnd_index = random.randint(0, X.shape[0])
    input = X[rnd_index]
    test_labels_p = torch.argmax(input).item()
    print(f"Initial letter: {[int_to_char[test_labels_p]]}")
    print("")
    print("Generated sequence: ", end="")
    list_of_char = []
    for i in range(50):
        # generate the letter in one hot encoding
        test_labels_onehot_p = model(input)
        # find the actual letter
        test_labels_p = torch.argmax(test_labels_onehot_p).item()
        list_of_char.append(int_to_char[test_labels_p])
        # recompute the one hot encoding to provide the next input
        # I recompute it because it seems to work better
        labels_one_hot = torch.zeros_like(test_labels_onehot_p)
        labels_one_hot[test_labels_p] = 1
        input = labels_one_hot
    # print(list_of_char)

    for char in list_of_char:
        # If the character is not a newline character, print it
        if char != '\n':
            print(char, end='')
        # If it is a newline character, print a newline
        else:
            print()

class WordNet(nn.Module):
    def __init__(self):
        super(WordNet, self).__init__()
        # 1st layer
        self.dense1 = nn.Linear(in_features=7, out_features=16)
        self.act1 = nn.ReLU()
        # 2nd layer
        self.dense2 = nn.Linear(in_features=16, out_features=7)

    # x represents our data
    def forward(self, x):
        x = self.dense1(x)
        x = self.act1(x)
        x = self.dense2(x)
        return x

model = WordNet()
# summary(model, (1,6))

num_epochs = 1001
# loss_fn = torch.nn.MSELoss()
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
# Transform the data into a tensor
train_matrix_tensor = torch.tensor(train_matrix, dtype=torch.float32)

# I'm using all the data, of course I'm giving the last index-1
X_train, Y_train = train_test_data(train_matrix_tensor,0,145060)

# Train the model
for epoch in range(num_epochs):
    # Predict the data
    Y_pred = model(X_train)
    # Compute the loss function (CrossEntropyLoss)
    loss = loss_fn(Y_pred, Y_train)
    # Zero all the gradients because I want to calculate them again
    optimizer.zero_grad()
    # Make backward pass
    loss.backward()
    # Take optimization step: update
    optimizer.step()
    # Print every 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: loss {loss.item()}")

# Compute the accuracy of the model
test_labels_onehot_p = model(X_train[100:200])
_, y_pred_labels = torch.max(test_labels_onehot_p, dim=1)
_, y_true_labels = torch.max(Y_train[100:200], dim=1)
# print(y_pred_labels)
# print(y_true_labels)
# num_correct = torch.sum(y_pred_labels == y_true_labels).item()
# accuracy = num_correct / Y_train[100:200].size(0)
# print(f"Accuracy: {accuracy}")
print("")
# Generate the 50 predict values
evaluation(X_train)




