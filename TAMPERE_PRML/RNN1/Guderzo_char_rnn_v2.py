import torch
import torch.nn.functional as F
import torch.nn as nn
from torchsummary import summary
from Guderzo_encoding import convert_to_hot
import random

train_matrix, char_to_int, int_to_char = convert_to_hot("C:/Users/ricca/Desktop/RGud/TAMPERE_UNIVERSITY/2_SEMESTER/PATTERN RECOGNITION AND MACHINE LEARNING/EXERCISE5/abcde_edcba.txt")

# I modified the function a little so to concatenate the values at t and t-1
def train_test_data(original_matrix=torch.rand(1, 7), index_from=0,
                    index_to=1):
    """
    Given a dataset, split it into X and Y based on the request. The X is the
    tensor concatenation between values at t and t-1.

    :param original_matrix:
    :param index_from:
    :param index_to:
    :return: X, Y training data
    """
    X_1 = original_matrix[index_from:index_to]
    X_2 = original_matrix[index_from-1:index_to-1]
    X = torch.cat([X_1, X_2], dim=1)
    Y = original_matrix[index_from + 1:index_to + 1]

    return X, Y

def evaluation(X):
    """
    Given a matrix of data, randomly choose a char and print the next 50 chars
    generated by the NN
    :param X: tensor concatenation between letters at t and t-1
    :return:
    """
    # print(X[0:20])
    rnd_index = random.randint(0, X.shape[0])
    input = X[rnd_index]
    test_labels_p_1 = torch.argmax(input[0:7]).item()
    test_labels_p_2 = torch.argmax(input[7:14]).item()
    print(f"Initial letters: {[int_to_char[test_labels_p_2], int_to_char[test_labels_p_1]]}")
    print("")
    print("Generated sequence: ", end="")
    list_of_char = []
    for i in range(50):
        test_labels_onehot_p = model(input)
        test_labels_p = torch.argmax(test_labels_onehot_p).item()
        list_of_char.append(int_to_char[test_labels_p])
        # I concatenate the new output (t+1) with previous t
        # t-1 is of course discarded
        labels_one_hot = torch.zeros_like(test_labels_onehot_p)
        labels_one_hot[test_labels_p] = 1
        # concatenate(xt, xt-1)
        input = torch.cat((labels_one_hot, input[0:7]), dim=0)
    # print(list_of_char)

    for char in list_of_char:
        # If the character is not a newline character, print it
        if char != '\n':
            print(char, end='')
        # If it is a newline character, print a newline
        else:
            print()

class WordNet(nn.Module):
    def __init__(self):
        super(WordNet, self).__init__()
        # 1st layer
        self.dense1 = nn.Linear(in_features=14, out_features=64)
        self.act1 = nn.ReLU()
        # 2nd layer
        self.dense2 = nn.Linear(in_features=64, out_features=7)

    # x represents our data
    def forward(self, x):
        x = self.dense1(x)
        x = self.act1(x)
        x = self.dense2(x)
        return x

model = WordNet()
# summary(model, (1,6))

num_epochs = 1001
# loss_fn = torch.nn.MSELoss()
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.07)
# Transform the data into a tensor
train_matrix_tensor = torch.tensor(train_matrix, dtype=torch.float32)

# I'm using all the data, of course I'm giving the last index-1
X_train, Y_train = train_test_data(train_matrix_tensor,1,145060)

# Train the model
for epoch in range(num_epochs):
    # Predict the data
    Y_pred = model(X_train)
    # Compute the loss function (CrossEntropyLoss)
    loss = loss_fn(Y_pred, Y_train)
    # Zero all the gradients because I want to calculate them again
    optimizer.zero_grad()
    # Make backward pass
    loss.backward()
    # Take optimization step: update
    optimizer.step()
    # Print every 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: loss {loss.item()}")

# Generate the 50 predict values
evaluation(X_train)
